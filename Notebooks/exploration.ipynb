{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "relative_path = '../'\n",
    "sys.path.append(relative_path)\n",
    "\n",
    "from Utils.generic_utils import get_reduced_name_list, tensor_to_images, plot_to_projector\n",
    "from Utils.trainer_utils import DataGenerator, compose_input\n",
    "from LatentCA.model import AutoEncoderDown2, NAFCA\n",
    "from NAFNet.model import NAFNet\n",
    "from Restormer.model import Restormer\n",
    "from ViTCA.model import ViTCA\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities to manage training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# line_styles for clear plots\n",
    "line_style_tuple = [('dotted', (0, (1, 1))), ('dashed', (0, (5, 5)))]\n",
    "\n",
    "# load data grouping by epoch and computing mean over it, then computing the total weighted loss\n",
    "def load_epoch_summary(filename, weights_dict):\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = pickle.load(f).groupby(by='epoch').mean()\n",
    "    d['total_loss'] = 0.\n",
    "    for key, value in weights_dict.items():\n",
    "        d['total_loss'] += value * d[key]\n",
    "    return d\n",
    "\n",
    "# plot single metric\n",
    "def plot_dataset(d, y, ax, x_label, y_label, real=True):\n",
    "    if real:\n",
    "        d.plot(y=y, ax=ax, kind='line', linestyle=line_style_tuple[0][1], xlabel=x_label, ylabel=y_label)\n",
    "    else:\n",
    "        d.plot(y=y, ax=ax, kind='line', linestyle=line_style_tuple[1][1], xlabel=x_label, ylabel=y_label)\n",
    "\n",
    "# plot all metrics\n",
    "def plot_all_dataset_wrapper(d_dict, y_list, x_label, y_label_list, size, max_x, save=False, filename=None):\n",
    "    rows, cols = 3, 2\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=size)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            for key, value in d_dict.items():\n",
    "                plot_dataset(value[0], y_list[i * cols + j], ax[i, j], x_label, y_label_list[i * cols + j], value[1])\n",
    "            ax[i, j].legend(list(d_dict.keys()), prop={'size': 10})\n",
    "            ax[i, j].grid(visible=True, alpha=0.2)\n",
    "            ax[i, j].set_xticks(np.arange(0, max_x, step=2))\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(filename, format='svg', dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "dataset_dict = {\n",
    "    'Blur': ['CelebA', 'CIFAR-10', 'GoPro', 'RealBlur', 'TinyImageNet'],\n",
    "    'Noise': ['CelebA', 'CIFAR-10', 'Renoir', 'SID', 'TinyImageNet']\n",
    "}\n",
    "results_folder = 'Results'\n",
    "model_name_list = ['LatentNAFCA', 'LatentViTCA', 'NAFNet', 'Restormer', 'ViTCA']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_dict = {'task': 'Noise', 'range': 1., 'intensity': 0.5, 'a_min': 0.25, 'a_max': 0.75}\n",
    "# task_dict = {'task': 'Blur', 'range': 15, 'intensity': 7, 'a_min': 4, 'a_max': 10}\n",
    "model_name = model_name_list[1]\n",
    "file_name = 'val_log.pickle'\n",
    "weights = {'rec_loss': 100, 'dist_loss': 10, 'task_loss': 10, 'equiv_loss': 100}\n",
    "d_ae = {data: (load_epoch_summary(relative_path + os.path.join(results_folder, task_dict['task'], data, model_name, 'Logs', file_name),weights), False)\n",
    "        for data in dataset_dict[task_dict['task']]}\n",
    "\n",
    "plot_all_dataset_wrapper(d_dict=d_ae, y_list=['rec_loss', 'dist_loss', 'task_loss', 'equiv_loss', 'total_loss', 'SSIM'], x_label='epoch',\n",
    "                         y_label_list=['Reconstruction Loss', 'Distance Loss', 'Task Loss', 'Equivalent Loss', 'Total Loss', 'SSIM'], size=(20, 15),\n",
    "                         max_x=len(list(d_ae.values())[0][0]), save=False, filename=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CA plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = 'val_ca_log.pickle'\n",
    "weights = {'rec_loss': 100, 'latent_loss': 100, 'out_loss': 1, 'hid_loss': 1}\n",
    "d_ca = {data: (load_epoch_summary(relative_path + os.path.join(results_folder, task_dict['task'], data, model_name, 'Logs', file_name),weights), False)\n",
    "        for data in dataset_dict[task_dict['task']]}\n",
    "\n",
    "plot_all_dataset_wrapper(d_dict=d_ca, y_list=['rec_loss', 'latent_loss', 'out_loss', 'hid_loss', 'total_loss', 'SSIM'], x_label='epoch',\n",
    "                         y_label_list=['Reconstruction Loss', 'Latent Loss', 'Output Overflow Loss', 'Hidden Overflow Loss', 'Total Loss', 'SSIM'], size=(20, 15),\n",
    "                         max_x=len(list(d_ae.values())[0][0]), save=False, filename=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Latent space visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the generator for the specific dataset\n",
    "dataset = 'CelebA'\n",
    "dataset_directory = relative_path + 'Datasets/' + dataset\n",
    "img_shape = (8, 32, 32, 3)\n",
    "reduction = .01\n",
    "test_split = 0.8\n",
    "val_split = 0.8\n",
    "real_dataset = False\n",
    "train, val, test = get_reduced_name_list(path=dataset_directory, reduction=reduction, test_split=test_split, val_split=val_split, real_dataset=real_dataset)\n",
    "test_gen = DataGenerator(directory=dataset_directory, filenames=test, img_size=img_shape, task_info=task_dict, train_ca=False, shuffle=False, num_iterations=0,\n",
    "                         curriculum=False, train=False, real_dataset=real_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set up the parameters for the model\n",
    "max_depth = 16\n",
    "num_down_sampling = 2\n",
    "cell_in_channels = cell_out_channels = max_depth\n",
    "cell_hidden_channels = 32\n",
    "pool_shape = (img_shape[0], img_shape[1] // (2 ** num_down_sampling), img_shape[1] // (2 ** num_down_sampling),\n",
    "              cell_in_channels + cell_out_channels + cell_hidden_channels)\n",
    "latent_shape = (img_shape[1] // (2 ** num_down_sampling), img_shape[1] // (2 ** num_down_sampling), max_depth)\n",
    "latent_nafca_params = {\n",
    "    'class': NAFCA,\n",
    "    'localized_attention_neighbourhood': (3, 3),\n",
    "    'ffn_expand': 4,\n",
    "    'dropout': 0.1,\n",
    "    'embed_dim': 128,\n",
    "    'cell_in_channels': cell_in_channels,\n",
    "    'cell_out_channels': cell_out_channels,\n",
    "    'cell_hidden_channels': cell_hidden_channels\n",
    "}\n",
    "latent_vitca_params = {\n",
    "    'class': ViTCA,\n",
    "    'localized_attention_neighbourhood': (3, 3),\n",
    "    'patch_size': 1,\n",
    "    'pos_encoding_max_freq': 5,\n",
    "    'depth': 1,\n",
    "    'heads': 4,\n",
    "    'mlp_dim': 64,\n",
    "    'dropout': 0.0,\n",
    "    'embed_cells': True,\n",
    "    'embed_dim': 128,\n",
    "    'embed_dropout': 0.0,\n",
    "    'pool_shape': pool_shape,\n",
    "    'cell_in_channels': cell_in_channels,\n",
    "    'cell_out_channels': cell_out_channels,\n",
    "    'cell_hidden_channels': cell_hidden_channels\n",
    "}\n",
    "latent_nafca_params_complete = {\n",
    "    'input_shape': img_shape,\n",
    "    'pool_shape': pool_shape,\n",
    "    'latent_shape': latent_shape,\n",
    "    'CAParams': None,\n",
    "    'Conv2DParams1': {\n",
    "        'filters': max_depth // 2,\n",
    "        'kernel_size': (3, 3),\n",
    "        'strides': (2, 2),\n",
    "        'padding': 'same',\n",
    "        'activation': 'swish',\n",
    "        'batch_normalization': True,\n",
    "        'dropout_rate': 0.0\n",
    "    },\n",
    "    'Conv2DParams2': {\n",
    "        'filters': max_depth,\n",
    "        'kernel_size': (3, 3),\n",
    "        'strides': (2, 2),\n",
    "        'padding': 'same',\n",
    "        'activation': 'swish',\n",
    "        'batch_normalization': True,\n",
    "        'dropout_rate': 0.0\n",
    "    },\n",
    "    'PassThroughParams1': {\n",
    "        'filters': max_depth // 2,\n",
    "        'kernel_size': (3, 3),\n",
    "        'strides': (2, 2),\n",
    "        'padding': 'same',\n",
    "        'activation': 'swish',\n",
    "        'batch_normalization': True,\n",
    "        'dropout_rate': 0.0\n",
    "    },\n",
    "    'Conv2DTransposeParams2': {\n",
    "        'filters': max_depth // 2,\n",
    "        'kernel_size': (3, 3),\n",
    "        'strides': (2, 2),\n",
    "        'padding': 'same',\n",
    "        'activation': 'swish',\n",
    "        'batch_normalization': True,\n",
    "        'dropout_rate': 0.0\n",
    "    },\n",
    "    'MixParams': {\n",
    "        'filters': max_depth // 2,\n",
    "        'kernel_size': (3, 3),\n",
    "        'strides': (1, 1),\n",
    "        'padding': 'same',\n",
    "        'activation': 'swish',\n",
    "        'batch_normalization': True,\n",
    "        'dropout_rate': 0.0\n",
    "    },\n",
    "    'Conv2DTransposeParams1': {\n",
    "        'filters': 3,\n",
    "        'kernel_size': (3, 3),\n",
    "        'strides': (2, 2),\n",
    "        'padding': 'same',\n",
    "        'activation': 'sigmoid',\n",
    "        'batch_normalization': True,\n",
    "        'dropout_rate': 0.0\n",
    "    }\n",
    "}\n",
    "latent_vitca_params_complete = copy.deepcopy(latent_nafca_params_complete)\n",
    "latent_nafca_params_complete['CAParams'] = latent_nafca_params\n",
    "latent_vitca_params_complete['CAParams'] = latent_vitca_params\n",
    "\n",
    "# load the pre-trained model\n",
    "model = AutoEncoderDown2(latent_vitca_params_complete)\n",
    "model.load_weights(relative_path + os.path.join(results_folder, task_dict['task'], dataset, model_name_list[1], 'Checkpoints', 'Best', 'weights'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the latent space\n",
    "if not os.path.exists('Projector/'):\n",
    "    os.mkdir('Projector/')\n",
    "plot_to_projector(model=model, generator=test_gen, path='Projector/', num_sample=7, noise=task_dict['task'] == 'Noise',\n",
    "                  min_value=task_dict['a_min'], max_value=task_dict['a_max'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To visualize open a terminal and use the command **tensorboard --logdir=Notebooks/Projector/**\n",
    "Then select the Projector tab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test results visualization between all models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load all test results for the selected dataset\n",
    "dataset = 'CelebA'\n",
    "metrics = ['PSNR', 'SSIM', 'REL_PSNR', 'REL_SSIM']\n",
    "df_list = []\n",
    "for name in model_name_list:\n",
    "    with open(relative_path + os.path.join(results_folder, task_dict['task'], dataset, name, 'Logs', 'test_log.pickle'), 'rb') as f:\n",
    "        df_list.append(pickle.load(f).mean()[metrics])\n",
    "df = pd.DataFrame(df_list, index=model_name_list)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visual comparison between all models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# set up the parameters for the remaining models\n",
    "nafnet_params = {\n",
    "    'input_shape': img_shape,\n",
    "    'width': 64,\n",
    "    'enc_block_nums': [2, 2, 4, 8],\n",
    "    'middle_block_num': 12,\n",
    "    'dec_block_nums': [2, 2, 2, 2]\n",
    "}\n",
    "restormer_params = {\n",
    "    'input_shape': img_shape,\n",
    "    'out_channels': 3,\n",
    "    'dim': 48,\n",
    "    'num_blocks': [4, 6, 6, 8],\n",
    "    'num_refinement_blocks': 4,\n",
    "    'heads': [1, 2, 4, 8],\n",
    "    'ffn_expansion_factor': 2.66,\n",
    "    'bias': False\n",
    "}\n",
    "vitca_params = {\n",
    "    'input_shape': img_shape,\n",
    "    'localized_attention_neighbourhood': [3, 3],\n",
    "    'patch_size': 1,\n",
    "    'overlapping_patches': False,\n",
    "    'pos_encoding_method': 'vit_handcrafted',\n",
    "    'pos_encoding_basis': 'raw_xy',\n",
    "    'pos_encoding_max_freq': 5,\n",
    "    'depth': 1,\n",
    "    'heads': 4,\n",
    "    'mlp_dim': 64,\n",
    "    'dropout': 0.0,\n",
    "    'cell_init': 'constant',\n",
    "    'cell_in_channels': img_shape[-1],\n",
    "    'cell_out_channels': img_shape[-1],\n",
    "    'cell_hidden_channels': cell_hidden_channels,\n",
    "    'embed_cells': True,\n",
    "    'embed_dim': 128,\n",
    "    'embed_dropout': 0.0\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load all the pre-trained models\n",
    "latent_nafca_model = AutoEncoderDown2(latent_nafca_params_complete)\n",
    "latent_nafca_model.load_weights(relative_path + os.path.join(results_folder, task_dict['task'], dataset, model_name_list[0], 'Checkpoints', 'Best', 'weights'))\n",
    "latent_vitca_model = AutoEncoderDown2(latent_vitca_params_complete)\n",
    "latent_vitca_model.load_weights(relative_path + os.path.join(results_folder, task_dict['task'], dataset, model_name_list[1], 'Checkpoints', 'Best', 'weights'))\n",
    "nafnet_model = NAFNet(nafnet_params)\n",
    "nafnet_model.load_weights(relative_path + os.path.join(results_folder, task_dict['task'], dataset, model_name_list[2], 'Checkpoints', 'Best', 'weights'))\n",
    "restormer_model = Restormer(restormer_params)\n",
    "restormer_model.load_weights(relative_path + os.path.join(results_folder, task_dict['task'], dataset, model_name_list[3], 'Checkpoints', 'Best', 'weights'))\n",
    "vitca_model = ViTCA(vitca_params)\n",
    "vitca_model.load_weights(relative_path + os.path.join(results_folder, task_dict['task'], dataset, model_name_list[4], 'Checkpoints', 'Best', 'weights'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# sample a batch from the generator and perform inference\n",
    "test_gen.switch_mode(True)\n",
    "anchor, positive = test_gen.__getitem__(0)\n",
    "latent_nafca_images = tensor_to_images(latent_nafca_model(compose_input(positive, pool_shape, 64, 0.5, None, False, True), training=False)[0])\n",
    "latent_vitca_images = tensor_to_images(latent_vitca_model(compose_input(positive, pool_shape, 64, 0.5, None, False, True), training=False)[0])\n",
    "nafnet_images = tensor_to_images(nafnet_model(positive, training=False))\n",
    "restormer_images = tensor_to_images(restormer_model(positive, training=False))\n",
    "vitca_images = tensor_to_images(vitca_model.get_rgb_out(vitca_model([vitca_model.seed(positive), tf.constant(64, dtype=tf.float32, shape=(1,)),\n",
    "                                                                     tf.constant(0.5, dtype=tf.float32, shape=(1,))], training=False)))\n",
    "anchor_images = tensor_to_images(anchor)\n",
    "positive_images = tensor_to_images(positive)\n",
    "model_images = [latent_nafca_images, latent_vitca_images, nafnet_images, restormer_images, vitca_images]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualize the first \"rows\" results\n",
    "rows = 5\n",
    "cols = len(model_name_list) + 2\n",
    "col_names = ['Ground Truth', 'Corrupted Image', *model_name_list]\n",
    "f, ax = plt.subplots(rows, cols, figsize=(cols * 10, rows * 10))\n",
    "for i in range(rows):\n",
    "    ax[i, 0].imshow(anchor_images[i])\n",
    "    ax[i, 0].axis('off')\n",
    "    ax[i, 1].imshow(positive_images[i])\n",
    "    ax[i, 1].axis('off')\n",
    "    for pos, value in enumerate(model_images):\n",
    "        ax[i, pos + 2].imshow(value[i])\n",
    "        ax[i, pos + 2].axis('off')\n",
    "for ax, col in zip(ax[0], col_names):\n",
    "    ax.set_title(col, fontsize = 48)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
